# Total data nums:
# BigFACAD: 164555
# FashionGen: 60147
# Fashion200k: 76856
# PolyvoreOutfits: 71967
# Total: 373408

includes:
- configs/models/FashionFAE/defaults.yaml

dataset_config:
  fashionall:
    attribute_label: true
    double_view: true
    processors:
        masked_image_processor:
          type: blockwise_masked_patch
        masked_token_processor:
          type: FashionFAE_text_tokenizer
          params:
            tokenizer_config:
              type: bert-base-uncased
              params:
                do_lower_case: true
            mask_probability: 0.15
            max_seq_length: 75
            do_whole_word_mask: true

model_config:
  FashionFAE:
    no_sharing: false
    image_encoder:
      type: torchvision_resnet
      params:
        name: resnet50
        pretrained: true
        zero_init_residual: false
        num_output_features: -1
        pool_type: avg
    image_tokenizer:
      type: vqvae_encoder
      params:
        pretrained_path: ${env.data_dir}/pretrained_models/vqvae_ema_pp_224_7x7_encoder_fashionall.pth
        resolution: 224
        num_tokens: 1024
        codebook_dim: 256
        attn_resolutions: [7]
        hidden_dim: 128
        in_channels: 3
        ch_mult: [1, 1, 2, 2, 4, 4]
        num_res_blocks: 2
        dropout: 0
        z_channels: 256
        double_z: false
    direct_features_input: false
    bypass_transformer: true
    bert_model_name: bert-base-uncased
    training_head_type: pretraining
    task_for_inference: itc
    tasks:
      - itm
      - pac
      - itc
      - icc
      - mlm
      - mpfc
    tasks_sample_ratio:
      - 0.15
      - 0.15
      - 0.2
      - 0.2
      - 0.15
      - 0.15

scheduler:
  type: multi_step
  params:
    use_warmup: true
    lr_steps:
    - 45000
    - 90000
    lr_ratio: 0.1
    warmup_iterations: 15000
    warmup_factor: 0.25

optimizer:
  type: adam_w
  params:
    lr: 1e-5
    eps: 1e-8
    weight_decay: 1e-4

evaluation:
  metrics:
  - r@k_general
  - r@k_rank

training:
  experiment_name: FashionFAE_e2e_pretrain_final
  batch_size: 256
  lr_scheduler: true
  max_updates: 120000
  log_interval: 10
  checkpoint_interval: 15000
  evaluation_interval: 1500
  find_unused_parameters: true
  early_stop:
    criteria: fashionall/r@k_general/avg
    minimize: false
  wandb:
    enabled: true

run_type: train_val
